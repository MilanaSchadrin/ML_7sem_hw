{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNQ-I1PUU-PH"
   },
   "source": [
    "# hw3 - ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ENj9WzCU-SK"
   },
   "source": [
    "## 1 Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MDgqWu9U-U-"
   },
   "source": [
    "Загрузите и предобработайте данные (по своему усмотрению) из hw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {
    "id": "IbSGi7csVlYZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 454 entries, 0 to 453\n",
      "Data columns (total 31 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   school      454 non-null    object \n",
      " 1   sex         454 non-null    object \n",
      " 2   age         426 non-null    float64\n",
      " 3   address     449 non-null    object \n",
      " 4   famsize     454 non-null    object \n",
      " 5   Pstatus     454 non-null    object \n",
      " 6   Medu        454 non-null    int64  \n",
      " 7   Fedu        454 non-null    int64  \n",
      " 8   Mjob        454 non-null    object \n",
      " 9   Fjob        454 non-null    object \n",
      " 10  reason      454 non-null    object \n",
      " 11  guardian    454 non-null    object \n",
      " 12  traveltime  454 non-null    int64  \n",
      " 13  studytime   454 non-null    int64  \n",
      " 14  failures    454 non-null    int64  \n",
      " 15  schoolsup   454 non-null    object \n",
      " 16  famsup      454 non-null    object \n",
      " 17  paid        454 non-null    object \n",
      " 18  activities  454 non-null    object \n",
      " 19  nursery     454 non-null    object \n",
      " 20  higher      454 non-null    object \n",
      " 21  internet    454 non-null    object \n",
      " 22  romantic    454 non-null    object \n",
      " 23  famrel      454 non-null    int64  \n",
      " 24  freetime    454 non-null    int64  \n",
      " 25  goout       454 non-null    int64  \n",
      " 26  Dalc        454 non-null    int64  \n",
      " 27  Walc        454 non-null    int64  \n",
      " 28  health      454 non-null    int64  \n",
      " 29  absences    454 non-null    int64  \n",
      " 30  G3          454 non-null    int64  \n",
      "dtypes: float64(1), int64(13), object(17)\n",
      "memory usage: 110.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train_features_with_answers.csv')\n",
    "#df.describe()\n",
    "df['age'].value_counts()\n",
    "df['age'] = df['age'].replace(161,16)\n",
    "df['age'] = df['age'].replace(181,18)\n",
    "df['age'] = df['age'].replace(151,15)\n",
    "df['age'] = df['age'].replace(116,16)\n",
    "df['age'] = df['age'].replace(5,15)\n",
    "df['age'] = df['age'].replace(8,18)\n",
    "df['age'] = df['age'].replace(1,16)\n",
    "df['age']=df['age'].replace(-1,17)\n",
    "df.info()\n",
    "nf = df.copy()\n",
    "y = nf['G3'].to_numpy().astype(float)\n",
    "X = nf.drop(['G3'], axis=1)\n",
    "#X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "#imputer = KNNImputer(n_neighbors=2)\n",
    "X['age']=imputer.fit_transform(X[['age']])\n",
    "mode = X['address'].mode().values[0]\n",
    "X['address'].fillna(value=mode, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остальная обработка, включая кодирование, дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Em0erMH6WyEV"
   },
   "source": [
    "## 2 Обоснуйте выбор слабых (базовых) алгоритмов\n",
    "Нужно выбрать разные алгоритмы, так как итоговый прогноз будет строится на основании их прогнозов, следовательно чем больше разнообразных по природе, тем лучше.\n",
    "LogisticRegression для простых линейных зависимостей.\n",
    "SVC - на прошлой домашке была наиболее достоверной вероятно из-за нелинейности.\n",
    "GaussianNB - работает на вероятностях. Сам по себе даёт худший результат, но с ним MSE уменьшается на 5.\n",
    "DecisionTreeClassifier - дерево.\n",
    "GB - минимизация ошибок по квадрату."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def get_models():\\n    models = []\\n    models.append(('lr', LinearRegression()))\\n    models.append(('knn', KNeighborsRegressor(n_neighbors=11)))\\n    models.append(('cart', DecisionTreeRegressor(max_depth=3)))\\n    models.append(('svm', SVR(kernel='linear', C=1)))\\n    return models\""
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def get_models():\n",
    "    models = []\n",
    "    models.append(('lr', LinearRegression()))\n",
    "    models.append(('knn', KNeighborsRegressor(n_neighbors=11)))\n",
    "    models.append(('cart', DecisionTreeRegressor(max_depth=3)))\n",
    "    models.append(('svm', SVR(kernel='linear', C=1)))\n",
    "    return models\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X1=X.copy()\n",
    "t1=['age','absences','studytime', 'failures', 'famrel']\n",
    "l1=['school','sex', 'Mjob','Fjob','reason','guardian','higher']\n",
    "for j in t1:\n",
    "    X1[j] = scaler.fit_transform(X1[[j]])\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "for i in l1:\n",
    "    transformed_X1 = encoder.fit_transform(X1[[i]])\n",
    "    encoded_X1 = pd.DataFrame(transformed_X1, columns=encoder.get_feature_names_out([i]))\n",
    "    X1 = pd.concat([X1, encoded_X1], axis=1)\n",
    "    X1.drop(i, axis=1, inplace=True)\n",
    "X1.drop('Pstatus', axis=1, inplace=True)\n",
    "X1.drop('paid', axis=1, inplace=True)\n",
    "X1.drop('famsize', axis=1, inplace=True)\n",
    "X1.drop('address', axis=1, inplace=True)\n",
    "X1.drop('Medu', axis=1, inplace=True)\n",
    "X1.drop('Fedu', axis=1, inplace=True)\n",
    "X1.drop('traveltime', axis=1, inplace=True)\n",
    "X1.drop('schoolsup', axis=1, inplace=True)\n",
    "X1.drop('famsup', axis=1, inplace=True)\n",
    "X1.drop('activities', axis=1, inplace=True)\n",
    "X1.drop('nursery', axis=1, inplace=True)\n",
    "X1.drop('internet', axis=1, inplace=True)\n",
    "X1.drop('romantic', axis=1, inplace=True)\n",
    "X1.drop('freetime', axis=1, inplace=True)\n",
    "X1.drop('goout', axis=1, inplace=True)\n",
    "X1.drop('Dalc', axis=1, inplace=True)\n",
    "X1.drop('Walc', axis=1, inplace=True)\n",
    "X1.drop('health', axis=1, inplace=True)\n",
    "X1=X1.to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          age      Medu      Fedu  traveltime  studytime  failures    famrel  \\\n",
      "0    0.176788 -1.333406  0.653495   -0.772553  -1.097453 -0.383276  0.095112   \n",
      "1    0.176788  0.452335  0.653495    1.806410  -1.097453 -0.383276 -0.958082   \n",
      "2   -1.461209  0.452335  1.583548   -0.772553   0.135823 -0.383276  1.148306   \n",
      "3    0.176788  1.345206  0.653495    0.516929   0.135823  1.289872  0.095112   \n",
      "4    2.633783 -0.440535 -1.206610    0.516929   0.135823 -0.383276 -3.064470   \n",
      "..        ...       ...       ...         ...        ...       ...       ...   \n",
      "449 -1.461209  1.345206 -0.276557   -0.772553   2.602377 -0.383276 -0.958082   \n",
      "450 -1.461209 -0.440535 -0.276557   -0.772553   2.602377 -0.383276  1.148306   \n",
      "451 -0.642210  1.345206  1.583548   -0.772553  -1.097453 -0.383276  1.148306   \n",
      "452 -1.461209 -1.333406 -1.206610    0.516929   0.135823 -0.383276  1.148306   \n",
      "453 -1.461209  1.345206  1.583548   -0.772553  -1.097453 -0.383276  1.148306   \n",
      "\n",
      "     freetime     goout      Dalc  ...  sex_A  sex_B  sex_C  sex_D  sex_F  \\\n",
      "0   -0.169875 -0.188474  0.499637  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "1   -0.169875  0.658728  1.540167  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "2    1.688515 -1.882879 -0.540892  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "3    1.688515  1.505930 -0.540892  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "4   -1.099070 -0.188474 -0.540892  ...    0.0    0.0    0.0    0.0    1.0   \n",
      "..        ...       ...       ...  ...    ...    ...    ...    ...    ...   \n",
      "449 -0.169875 -0.188474 -0.540892  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "450 -2.028265 -1.035677 -0.540892  ...    0.0    0.0    0.0    0.0    1.0   \n",
      "451 -0.169875 -1.035677 -0.540892  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "452  0.759320 -0.188474 -0.540892  ...    0.0    0.0    0.0    0.0    1.0   \n",
      "453 -0.169875 -0.188474 -0.540892  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "     sex_M  address_R  address_U  famsize_GT3  famsize_LE3  \n",
      "0      1.0        0.0        1.0          0.0          1.0  \n",
      "1      1.0        1.0        0.0          0.0          1.0  \n",
      "2      1.0        0.0        1.0          1.0          0.0  \n",
      "3      1.0        1.0        0.0          1.0          0.0  \n",
      "4      0.0        1.0        0.0          1.0          0.0  \n",
      "..     ...        ...        ...          ...          ...  \n",
      "449    1.0        0.0        1.0          1.0          0.0  \n",
      "450    0.0        0.0        1.0          1.0          0.0  \n",
      "451    1.0        0.0        1.0          1.0          0.0  \n",
      "452    0.0        1.0        0.0          0.0          1.0  \n",
      "453    1.0        0.0        1.0          1.0          0.0  \n",
      "\n",
      "[454 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "X2=X.copy()\n",
    "t2=['age','Medu','Fedu','traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health']\n",
    "l2=['sex','address','famsize']\n",
    "for j in t2:\n",
    "    X2[j] = scaler.fit_transform(X2[[j]])\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "for i in l2:\n",
    "    transformed_X2 = encoder.fit_transform(X2[[i]])\n",
    "    encoded_X2 = pd.DataFrame(transformed_X2, columns=encoder.get_feature_names_out([i]))\n",
    "    X2 = pd.concat([X2, encoded_X2], axis=1)\n",
    "    X2.drop(i, axis=1, inplace=True)\n",
    "X2.drop('Pstatus', axis=1, inplace=True)\n",
    "X2.drop('paid', axis=1, inplace=True)\n",
    "X2.drop('school', axis=1, inplace=True)\n",
    "X2.drop('Mjob', axis=1, inplace=True)\n",
    "X2.drop('Fjob', axis=1, inplace=True)\n",
    "X2.drop('schoolsup', axis=1, inplace=True)\n",
    "X2.drop('famsup', axis=1, inplace=True)\n",
    "X2.drop('activities', axis=1, inplace=True)\n",
    "X2.drop('nursery', axis=1, inplace=True)\n",
    "X2.drop('internet', axis=1, inplace=True)\n",
    "X2.drop('romantic', axis=1, inplace=True)\n",
    "X2.drop('reason', axis=1, inplace=True)\n",
    "X2.drop('guardian', axis=1, inplace=True)\n",
    "X2.drop('higher', axis=1, inplace=True)\n",
    "print(X2)\n",
    "X2=X2.to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     school_GP  school_MS  sex_A  sex_B  sex_C  sex_D  sex_F  sex_M  \\\n",
      "0          0.0        1.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
      "1          1.0        0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
      "2          1.0        0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
      "3          0.0        1.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
      "4          1.0        0.0    0.0    0.0    0.0    0.0    1.0    0.0   \n",
      "..         ...        ...    ...    ...    ...    ...    ...    ...   \n",
      "449        1.0        0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
      "450        1.0        0.0    0.0    0.0    0.0    0.0    1.0    0.0   \n",
      "451        1.0        0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
      "452        0.0        1.0    0.0    0.0    0.0    0.0    1.0    0.0   \n",
      "453        1.0        0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
      "\n",
      "     address_R  address_U  ...  higher_no  higher_yes  nursery_no  \\\n",
      "0          0.0        1.0  ...        1.0         0.0         0.0   \n",
      "1          1.0        0.0  ...        0.0         1.0         0.0   \n",
      "2          0.0        1.0  ...        0.0         1.0         0.0   \n",
      "3          1.0        0.0  ...        0.0         1.0         1.0   \n",
      "4          1.0        0.0  ...        1.0         0.0         0.0   \n",
      "..         ...        ...  ...        ...         ...         ...   \n",
      "449        0.0        1.0  ...        0.0         1.0         0.0   \n",
      "450        0.0        1.0  ...        0.0         1.0         0.0   \n",
      "451        0.0        1.0  ...        0.0         1.0         0.0   \n",
      "452        1.0        0.0  ...        0.0         1.0         0.0   \n",
      "453        0.0        1.0  ...        0.0         1.0         1.0   \n",
      "\n",
      "     nursery_yes  internet_no  internet_yes  romantic_no  romantic_yes  \\\n",
      "0            1.0          0.0           1.0          0.0           1.0   \n",
      "1            1.0          0.0           1.0          1.0           0.0   \n",
      "2            1.0          0.0           1.0          1.0           0.0   \n",
      "3            0.0          0.0           1.0          0.0           1.0   \n",
      "4            1.0          0.0           1.0          0.0           1.0   \n",
      "..           ...          ...           ...          ...           ...   \n",
      "449          1.0          0.0           1.0          1.0           0.0   \n",
      "450          1.0          0.0           1.0          1.0           0.0   \n",
      "451          1.0          0.0           1.0          1.0           0.0   \n",
      "452          1.0          0.0           1.0          1.0           0.0   \n",
      "453          0.0          0.0           1.0          1.0           0.0   \n",
      "\n",
      "     paid_no  paid_yes  \n",
      "0        1.0       0.0  \n",
      "1        1.0       0.0  \n",
      "2        1.0       0.0  \n",
      "3        0.0       1.0  \n",
      "4        0.0       1.0  \n",
      "..       ...       ...  \n",
      "449      1.0       0.0  \n",
      "450      1.0       0.0  \n",
      "451      0.0       1.0  \n",
      "452      1.0       0.0  \n",
      "453      0.0       1.0  \n",
      "\n",
      "[454 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "X4=X.copy()\n",
    "l4=['school','sex','address','famsize','Mjob','Fjob','reason','guardian','schoolsup','famsup','activities','higher','nursery','internet','romantic','paid']\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "for i in l4:\n",
    "    transformed_X4 = encoder.fit_transform(X4[[i]])\n",
    "    encoded_X4 = pd.DataFrame(transformed_X4, columns=encoder.get_feature_names_out([i]))\n",
    "    X4 = pd.concat([X4, encoded_X4], axis=1)\n",
    "    X4.drop(i, axis=1, inplace=True)\n",
    "X4.drop('Pstatus', axis=1, inplace=True)\n",
    "X4.drop('age', axis=1, inplace=True)\n",
    "#X4.drop('paid', axis=1, inplace=True)\n",
    "X4.drop('Medu', axis=1, inplace=True)\n",
    "X4.drop('Fedu', axis=1, inplace=True)\n",
    "X4.drop('traveltime', axis=1, inplace=True)\n",
    "X4.drop('studytime', axis=1, inplace=True)\n",
    "X4.drop('failures', axis=1, inplace=True)\n",
    "X4.drop('famrel', axis=1, inplace=True)\n",
    "X4.drop('freetime', axis=1, inplace=True)\n",
    "X4.drop('goout', axis=1, inplace=True)\n",
    "X4.drop('Dalc', axis=1, inplace=True)\n",
    "X4.drop('Walc', axis=1, inplace=True)\n",
    "X4.drop('health', axis=1, inplace=True)\n",
    "X4.drop('absences', axis=1, inplace=True)\n",
    "print(X4)\n",
    "X4=X4.to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          age      Medu      Fedu  traveltime  studytime  failures    famrel  \\\n",
      "0    0.176788 -1.333406  0.653495   -0.772553  -1.097453 -0.383276  0.095112   \n",
      "1    0.176788  0.452335  0.653495    1.806410  -1.097453 -0.383276 -0.958082   \n",
      "2   -1.461209  0.452335  1.583548   -0.772553   0.135823 -0.383276  1.148306   \n",
      "3    0.176788  1.345206  0.653495    0.516929   0.135823  1.289872  0.095112   \n",
      "4    2.633783 -0.440535 -1.206610    0.516929   0.135823 -0.383276 -3.064470   \n",
      "..        ...       ...       ...         ...        ...       ...       ...   \n",
      "449 -1.461209  1.345206 -0.276557   -0.772553   2.602377 -0.383276 -0.958082   \n",
      "450 -1.461209 -0.440535 -0.276557   -0.772553   2.602377 -0.383276  1.148306   \n",
      "451 -0.642210  1.345206  1.583548   -0.772553  -1.097453 -0.383276  1.148306   \n",
      "452 -1.461209 -1.333406 -1.206610    0.516929   0.135823 -0.383276  1.148306   \n",
      "453 -1.461209  1.345206  1.583548   -0.772553  -1.097453 -0.383276  1.148306   \n",
      "\n",
      "     freetime     goout      Dalc      Walc    health  absences  \n",
      "0   -0.169875 -0.188474  0.499637  0.530324 -0.375232 -0.801979  \n",
      "1   -0.169875  0.658728  1.540167  2.054166 -0.375232  2.686390  \n",
      "2    1.688515 -1.882879 -0.540892 -0.993518  1.009771 -0.801979  \n",
      "3    1.688515  1.505930 -0.540892  0.530324 -1.067733  0.070113  \n",
      "4   -1.099070 -0.188474 -0.540892 -0.231597 -1.067733  0.942205  \n",
      "..        ...       ...       ...       ...       ...       ...  \n",
      "449 -0.169875 -0.188474 -0.540892 -0.993518 -0.375232 -0.801979  \n",
      "450 -2.028265 -1.035677 -0.540892 -0.993518 -0.375232  0.070113  \n",
      "451 -0.169875 -1.035677 -0.540892 -0.231597  1.009771  0.070113  \n",
      "452  0.759320 -0.188474 -0.540892 -0.231597  0.317269 -0.801979  \n",
      "453 -0.169875 -0.188474 -0.540892 -0.993518  1.009771 -0.365933  \n",
      "\n",
      "[454 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "X5=X.copy()\n",
    "t5=['age','absences','Medu','Fedu','traveltime','studytime','failures','freetime','famrel','goout','Dalc','Walc','health']\n",
    "for j in t5:\n",
    "    X5[j] = scaler.fit_transform(X5[[j]])\n",
    "X5.drop('Pstatus', axis=1, inplace=True)\n",
    "X5.drop('paid', axis=1, inplace=True)\n",
    "X5.drop('school', axis=1, inplace=True)\n",
    "X5.drop('Mjob', axis=1, inplace=True)\n",
    "X5.drop('Fjob', axis=1, inplace=True)\n",
    "X5.drop('schoolsup', axis=1, inplace=True)\n",
    "X5.drop('famsup', axis=1, inplace=True)\n",
    "X5.drop('activities', axis=1, inplace=True)\n",
    "X5.drop('nursery', axis=1, inplace=True)\n",
    "X5.drop('internet', axis=1, inplace=True)\n",
    "X5.drop('romantic', axis=1, inplace=True)\n",
    "X5.drop('reason', axis=1, inplace=True)\n",
    "X5.drop('guardian', axis=1, inplace=True)\n",
    "X5.drop('higher', axis=1, inplace=True)\n",
    "X5.drop('sex', axis=1, inplace=True)\n",
    "X5.drop('address', axis=1, inplace=True)\n",
    "X5.drop('famsize', axis=1, inplace=True)\n",
    "print(X5)\n",
    "X5=X5.to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          age      Medu      Fedu  traveltime  studytime  failures    famrel  \\\n",
      "0    0.176788 -1.333406  0.653495   -0.772553  -1.097453 -0.383276  0.095112   \n",
      "1    0.176788  0.452335  0.653495    1.806410  -1.097453 -0.383276 -0.958082   \n",
      "2   -1.461209  0.452335  1.583548   -0.772553   0.135823 -0.383276  1.148306   \n",
      "3    0.176788  1.345206  0.653495    0.516929   0.135823  1.289872  0.095112   \n",
      "4    2.633783 -0.440535 -1.206610    0.516929   0.135823 -0.383276 -3.064470   \n",
      "..        ...       ...       ...         ...        ...       ...       ...   \n",
      "449 -1.461209  1.345206 -0.276557   -0.772553   2.602377 -0.383276 -0.958082   \n",
      "450 -1.461209 -0.440535 -0.276557   -0.772553   2.602377 -0.383276  1.148306   \n",
      "451 -0.642210  1.345206  1.583548   -0.772553  -1.097453 -0.383276  1.148306   \n",
      "452 -1.461209 -1.333406 -1.206610    0.516929   0.135823 -0.383276  1.148306   \n",
      "453 -1.461209  1.345206  1.583548   -0.772553  -1.097453 -0.383276  1.148306   \n",
      "\n",
      "     freetime     goout      Dalc  ...  activities_no  activities_yes  \\\n",
      "0   -0.169875 -0.188474  0.499637  ...            1.0             0.0   \n",
      "1   -0.169875  0.658728  1.540167  ...            0.0             1.0   \n",
      "2    1.688515 -1.882879 -0.540892  ...            0.0             1.0   \n",
      "3    1.688515  1.505930 -0.540892  ...            0.0             1.0   \n",
      "4   -1.099070 -0.188474 -0.540892  ...            0.0             1.0   \n",
      "..        ...       ...       ...  ...            ...             ...   \n",
      "449 -0.169875 -0.188474 -0.540892  ...            1.0             0.0   \n",
      "450 -2.028265 -1.035677 -0.540892  ...            1.0             0.0   \n",
      "451 -0.169875 -1.035677 -0.540892  ...            0.0             1.0   \n",
      "452  0.759320 -0.188474 -0.540892  ...            1.0             0.0   \n",
      "453 -0.169875 -0.188474 -0.540892  ...            0.0             1.0   \n",
      "\n",
      "     higher_no  higher_yes  nursery_no  nursery_yes  internet_no  \\\n",
      "0          1.0         0.0         0.0          1.0          0.0   \n",
      "1          0.0         1.0         0.0          1.0          0.0   \n",
      "2          0.0         1.0         0.0          1.0          0.0   \n",
      "3          0.0         1.0         1.0          0.0          0.0   \n",
      "4          1.0         0.0         0.0          1.0          0.0   \n",
      "..         ...         ...         ...          ...          ...   \n",
      "449        0.0         1.0         0.0          1.0          0.0   \n",
      "450        0.0         1.0         0.0          1.0          0.0   \n",
      "451        0.0         1.0         0.0          1.0          0.0   \n",
      "452        0.0         1.0         0.0          1.0          0.0   \n",
      "453        0.0         1.0         1.0          0.0          0.0   \n",
      "\n",
      "     internet_yes  romantic_no  romantic_yes  \n",
      "0             1.0          0.0           1.0  \n",
      "1             1.0          1.0           0.0  \n",
      "2             1.0          1.0           0.0  \n",
      "3             1.0          0.0           1.0  \n",
      "4             1.0          0.0           1.0  \n",
      "..            ...          ...           ...  \n",
      "449           1.0          1.0           0.0  \n",
      "450           1.0          1.0           0.0  \n",
      "451           1.0          1.0           0.0  \n",
      "452           1.0          1.0           0.0  \n",
      "453           1.0          1.0           0.0  \n",
      "\n",
      "[454 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "t=['age','absences','Medu','Fedu','traveltime','studytime','failures','freetime','famrel','goout','Dalc','Walc','health']\n",
    "l = ['school','sex','address','famsize','Mjob','Fjob','reason','guardian','schoolsup','famsup','activities','higher','nursery','internet','romantic']\n",
    "for j in t:\n",
    "    X[j] = scaler.fit_transform(X[[j]])\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "l = ['school','sex','address','famsize','Mjob','Fjob','reason','guardian','schoolsup','famsup','activities','higher','nursery','internet','romantic']\n",
    "for i in l:\n",
    "    transformed_X = encoder.fit_transform(X[[i]])\n",
    "    encoded_X = pd.DataFrame(transformed_X, columns=encoder.get_feature_names_out([i]))\n",
    "    X = pd.concat([X, encoded_X], axis=1)\n",
    "    X.drop(i, axis=1, inplace=True)\n",
    "X.drop('Pstatus', axis=1, inplace=True)\n",
    "X.drop('paid', axis=1, inplace=True)\n",
    "print(X)\n",
    "X=X.to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующее не нужно, я не прочитала про вероятности, поэтому сделала регрессию. Ниже классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {
    "id": "KqC_5cbgWyML"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from scipy import stats\\nfrom sklearn.metrics import accuracy_score\\nimport numpy as np\\n\\ndef model(model, X_train, y_train, X_val, X_test, y_test):\\n    model.fit(X_train, y_train)\\n    valid = model.predict(X_val)\\n    prediction = model.predict(X_test)\\n    print(f' MSE: {mean_squared_error(y_test, prediction)}')\\n    return prediction, valid\\n\\n#Base models\\nfirst =  GradientBoostingRegressor(random_state=42, criterion = 'squared_error')\\nsecond = KNeighborsRegressor(n_neighbors=15)\\nthird = DecisionTreeRegressor(max_depth=2)\\nfourth = SVR(kernel='linear')\\n\\ns_m=pd.DataFrame(X_train_m).drop([4,17,31], axis=1).to_numpy().astype(float)\\ns_v=pd.DataFrame(X_val).drop([4,17,31], axis=1).to_numpy().astype(float)\\ns_t=pd.DataFrame(X_test).drop([4,17,31], axis=1).to_numpy().astype(float)\\n\\nt_m=pd.DataFrame(X_train_m).drop([6,42,27,46,38], axis=1).to_numpy().astype(float)\\nt_v=pd.DataFrame(X_val).drop([6,42,27,46,38], axis=1).to_numpy().astype(float)\\nt_t=pd.DataFrame(X_test).drop([6,42,27,46,38], axis=1).to_numpy().astype(float)\\n\\nl_m=pd.DataFrame(X_train_m).drop([52,11,54,32,24], axis=1).to_numpy().astype(float)\\nl_v=pd.DataFrame(X_val).drop([52,11,54,32,24], axis=1).to_numpy().astype(float)\\nl_t=pd.DataFrame(X_test).drop([52,11,54,32,24], axis=1).to_numpy().astype(float)\\n\\np0,v0 = model(first, X_train_m, y_train_m, X_val, X_test, y_test)\\np1,v1=model(second, s_m, y_train_m, s_v, s_t, y_test)\\np2,v2=model(third, t_m, y_train_m, t_v, t_t, y_test)\\np3,v3=model(fourth, l_m, y_train_m, l_v, l_t, y_test)\\n\\ns=np.isclose(v1, y_val, atol=0.3)  \\nt=np.isclose(v2, y_val, atol=0.3) \\nf=np.isclose(v3, y_val, atol=0.1)\\n\\nps=np.isclose(p1, y_test, atol=0.3)  \\npt=np.isclose(p2, y_test, atol=0.3) \\npf=np.isclose(p3, y_test, atol=0.3)\\n\\ndf_p=pd.concat([pd.DataFrame(p0), pd.DataFrame(p1),pd.DataFrame(p2),pd.DataFrame(p3)],axis=1)\\ndf_p=df_p.to_numpy().astype(float)\\ndf_v=pd.concat([pd.DataFrame(v0),pd.DataFrame(v1),pd.DataFrame(v2),pd.DataFrame(v3)],axis=1)\\ndf_v=df_v.to_numpy().astype(float)\""
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def model(model, X_train, y_train, X_val, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    valid = model.predict(X_val)\n",
    "    prediction = model.predict(X_test)\n",
    "    print(f' MSE: {mean_squared_error(y_test, prediction)}')\n",
    "    return prediction, valid\n",
    "\n",
    "#Base models\n",
    "first =  GradientBoostingRegressor(random_state=42, criterion = 'squared_error')\n",
    "second = KNeighborsRegressor(n_neighbors=15)\n",
    "third = DecisionTreeRegressor(max_depth=2)\n",
    "fourth = SVR(kernel='linear')\n",
    "\n",
    "s_m=pd.DataFrame(X_train_m).drop([4,17,31], axis=1).to_numpy().astype(float)\n",
    "s_v=pd.DataFrame(X_val).drop([4,17,31], axis=1).to_numpy().astype(float)\n",
    "s_t=pd.DataFrame(X_test).drop([4,17,31], axis=1).to_numpy().astype(float)\n",
    "\n",
    "t_m=pd.DataFrame(X_train_m).drop([6,42,27,46,38], axis=1).to_numpy().astype(float)\n",
    "t_v=pd.DataFrame(X_val).drop([6,42,27,46,38], axis=1).to_numpy().astype(float)\n",
    "t_t=pd.DataFrame(X_test).drop([6,42,27,46,38], axis=1).to_numpy().astype(float)\n",
    "\n",
    "l_m=pd.DataFrame(X_train_m).drop([52,11,54,32,24], axis=1).to_numpy().astype(float)\n",
    "l_v=pd.DataFrame(X_val).drop([52,11,54,32,24], axis=1).to_numpy().astype(float)\n",
    "l_t=pd.DataFrame(X_test).drop([52,11,54,32,24], axis=1).to_numpy().astype(float)\n",
    "\n",
    "p0,v0 = model(first, X_train_m, y_train_m, X_val, X_test, y_test)\n",
    "p1,v1=model(second, s_m, y_train_m, s_v, s_t, y_test)\n",
    "p2,v2=model(third, t_m, y_train_m, t_v, t_t, y_test)\n",
    "p3,v3=model(fourth, l_m, y_train_m, l_v, l_t, y_test)\n",
    "\n",
    "s=np.isclose(v1, y_val, atol=0.3)  \n",
    "t=np.isclose(v2, y_val, atol=0.3) \n",
    "f=np.isclose(v3, y_val, atol=0.1)\n",
    "\n",
    "ps=np.isclose(p1, y_test, atol=0.3)  \n",
    "pt=np.isclose(p2, y_test, atol=0.3) \n",
    "pf=np.isclose(p3, y_test, atol=0.3)\n",
    "\n",
    "df_p=pd.concat([pd.DataFrame(p0), pd.DataFrame(p1),pd.DataFrame(p2),pd.DataFrame(p3)],axis=1)\n",
    "df_p=df_p.to_numpy().astype(float)\n",
    "df_v=pd.concat([pd.DataFrame(v0),pd.DataFrame(v1),pd.DataFrame(v2),pd.DataFrame(v3)],axis=1)\n",
    "df_v=df_v.to_numpy().astype(float)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "#Base models\n",
    "first =  GradientBoostingClassifier(max_depth = 2, random_state=42, criterion = 'squared_error', learning_rate=0.01)\n",
    "second = KNeighborsClassifier(n_neighbors=15, algorithm= 'kd_tree')\n",
    "third = DecisionTreeClassifier(max_depth=2)\n",
    "fourth = SVC(kernel='rbf', probability=True, gamma=0.15)\n",
    "fifth = GaussianNB()\n",
    "sixth = LogisticRegression(random_state=0,solver='newton-cholesky',penalty='l2',C = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MSE: 10.191176470588236\n",
      " MSE: 9.470588235294118\n",
      " MSE: 9.058823529411764\n",
      " MSE: 8.75\n",
      " MSE: 21.264705882352942\n",
      " MSE: 9.676470588235293\n"
     ]
    }
   ],
   "source": [
    "def model(model, X_train, y_train, X_val, X_test, y_val):\n",
    "    model.fit(X_train, y_train)\n",
    "    valid = model.predict_proba(X_val)\n",
    "    prediction = model.predict(X_val)\n",
    "    print(f' MSE: {mean_squared_error(y_val, prediction)}')\n",
    "    proba = np.argmax(valid, axis=1)\n",
    "    #proba = (valid_proba >= 0.5).astype(int)\n",
    "    predi = model.predict_proba(X_test)\n",
    "    test =  np.argmax(predi, axis=1)\n",
    "    return proba, test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.25, random_state=42)\n",
    "X_train_m, X_val, y_train_m, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "v0,p0 = model(first, X_train_m, y_train_m, X_val, X_test, y_val)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.25, random_state=42)\n",
    "X_train_m, X_val, y_train_m, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "v1,p1 = model(second, X_train_m, y_train_m, X_val, X_test, y_val)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train_m, X_val, y_train_m, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "v2,p2 = model(third, X_train_m, y_train_m, X_val, X_test, y_val)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X4, y, test_size=0.25, random_state=42)\n",
    "X_train_m, X_val, y_train_m, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "v3,p3 = model(fourth, X_train_m, y_train_m, X_val, X_test, y_val)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X5, y, test_size=0.25, random_state=42)\n",
    "X_train_m, X_val, y_train_m, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "v4,p4 = model(fifth, X_train_m, y_train_m, X_val, X_test, y_val)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train_m, X_val, y_train_m, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "v5,p5 = model(sixth, X_train_m, y_train_m, X_val, X_test, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R08QEdnPU-X5"
   },
   "source": [
    "## 3 Постройте решение на основе подхода Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sF_TRbWxYQ8p"
   },
   "source": [
    "Правила:\n",
    "- Нужно использовать вероятности\n",
    "- Предложите что-то лучше, чем брать среднее от предсказаний моделей (оценивать уверенность алгоритмов, точности и т.д.)\n",
    "- Заставьте базовые алгоритмы быть некорелированными\n",
    "- Добавьте рандома (например, стройте ваши алгоритмы на разных выборках, по разному предобрабатывайте данные или применяйте для разных признаков соответствующие алгоритмы ... )\n",
    "- Проявите смекалку\n",
    "- Цель: метрика MSE на тесте меньше 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {
    "id": "I-Kw6bNqPUoj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MSE: 8.21750118533107\n",
      " MSE: 8.078947368421053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#blender = LinearRegression()\n",
    "v=pd.concat([pd.DataFrame(v0),pd.DataFrame(v1),pd.DataFrame(v2),pd.DataFrame(v4),pd.DataFrame(v5)],axis=1)\n",
    "p=pd.concat([pd.DataFrame(p0),pd.DataFrame(p1),pd.DataFrame(p2),pd.DataFrame(p4),pd.DataFrame(p5)],axis=1)\n",
    "blender = RandomForestRegressor (max_depth=2, random_state=0, criterion='squared_error', n_estimators=21)\n",
    "blender.fit(v, y_val)\n",
    "#blender.fit(v, y_val)\n",
    "prediction = blender.predict(p)\n",
    "predictionint = np.round(blender.predict(p)).astype(int)\n",
    "print(f' MSE: {mean_squared_error(y_test, prediction)}')\n",
    "print(f' MSE: {mean_squared_error(y_test, predictionint)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRmBr8VRWolP"
   },
   "source": [
    "## 4 Постройте решение на основе подхода Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oYYsgNRasfs"
   },
   "source": [
    "Правила:\n",
    "- Реализуйте пайплайн обучения и предсказания (например, sklearn.pipeline или класс)\n",
    "- Проведите оптимизацию пайплайна\n",
    "- Оцените вклад каждого базового алгоритма в итоговое предсказание\n",
    "- Цель: метрика MSE на тесте меньше 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train_m, X_val, y_train_m, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr=Pipeline([('scaler', StandardScaler()),\n",
    "                      ('lr_classifier',LogisticRegression(random_state=0,solver='newton-cholesky',penalty='l2',C = 0.15))])\n",
    "pipeline_dt=Pipeline([('scaler', StandardScaler()),\n",
    "                      ('dt_classifier',DecisionTreeClassifier(max_depth=3))])\n",
    "pipeline_svm = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('clf', SVC(kernel='rbf', probability=True, C=1.9))])\n",
    "pipeline_knn=Pipeline([('scaler', StandardScaler()),\n",
    "                       ('knn_classifier',KNeighborsClassifier(n_neighbors=15))])\n",
    "pipeline_gb=Pipeline([('scaler', StandardScaler()),\n",
    "                      ('rf_classifier', GradientBoostingClassifier(max_depth=2, random_state=42, criterion = 'squared_error', learning_rate=0.05))])\n",
    "pipeline_nb=Pipeline([('scaler', StandardScaler()),\n",
    "                      ('NB_classifier', GaussianNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "r_lr = cross_val_predict(pipeline_lr, X_train, y_train, \n",
    "                                    cv=KFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                                    method='predict_proba')\n",
    "r_dt = cross_val_predict(pipeline_dt, X_train, y_train, \n",
    "                                    cv=KFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                                    method='predict_proba')\n",
    "r_svm = cross_val_predict(pipeline_svm, X_train, y_train, \n",
    "                                    cv=KFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                                    method='predict_proba')\n",
    "r_knn = cross_val_predict(pipeline_knn, X_train, y_train, \n",
    "                                    cv=KFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                                    method='predict_proba')\n",
    "r_gb = cross_val_predict(pipeline_gb, X_train, y_train, \n",
    "                                    cv=KFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                                    method='predict_proba')\n",
    "r_lnb = cross_val_predict(pipeline_nb, X_train, y_train, \n",
    "                                    cv=KFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                                    method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = np.hstack([r_lr,r_dt,r_svm,r_knn,r_gb,r_lnb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [pipeline_lr, pipeline_dt, pipeline_svm, pipeline_knn, pipeline_rf, pipeline_nb]\n",
    "pipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'Support Vector Machine',3:'K Nearest Neighbor', 4: 'Gradient Boost', 5: 'Naive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MSE: 9.719298245614034\n",
      " MSE: 8.964912280701755\n",
      " MSE: 9.62280701754386\n",
      " MSE: 12.271929824561404\n",
      " MSE: 9.68421052631579\n",
      " MSE: 29.219298245614034\n"
     ]
    }
   ],
   "source": [
    "proba_res=[]\n",
    "mse=[]\n",
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train, y_train)\n",
    "for i,model in enumerate(pipelines):\n",
    "    prediction = model.predict(X_test)\n",
    "    proba = model.predict_proba(X_test)\n",
    "    proba_res.append(proba)\n",
    "    mse.append(mean_squared_error(y_test, prediction))\n",
    "    print(f' MSE: {mean_squared_error(y_test, prediction)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_t=np.hstack([proba_res[0],proba_res[1],\n",
    "                   proba_res[2],proba_res[3],\n",
    "                  proba_res[4],proba_res[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MSE: 7.793351413519016\n"
     ]
    }
   ],
   "source": [
    "stacker = RandomForestRegressor(max_depth=2, random_state=0, criterion='squared_error')\n",
    "stacker.fit(stack,y_train)\n",
    "prediction = stacker.predict(stack_t)\n",
    "print(f' MSE: {mean_squared_error(y_test, prediction)}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
